# Introduction
All information created for this project is available at [this Github repo](https://github.com/devonorourke/guano/tree/master/OahuBird). Please visit that page for more information regarding data tables, visualizations, and code used to complete this work.

## molecular work
> Alissa Scinto will update this extraction history...  

- include any information regarding the experimental design in which collections were performed (ie. what areas were the fecal samples collected, when were they collected, by whom were they collected, etc.)  
- include description of how samples were collected; what media they were stored in (ex. ethanol, air, RNAlater, etc.); whether they were frozen; how they were shipped
- include specific kit and protocols used to extract DNA (ex. MoBio? Qiagen? list any deviations from manufacturer protocol)  

- Provide details of PCR work; were all samples amplified in one round? are there negative or posiive controls? how were (amplicon) samples quantified? what target concentration were samples pooled? were samples primer dimers removed - if so, how?  

## sequencing at NAU
The two pooled libraries of COI amplicons were sequenced using a MiSeq platform following 300 bp PE sequencing using V3 chemistry set for 600 cycles Northern Arizona University's sequencing center on February 10, 2018 (p10-1) and February 25, 2018 (p10-2). Raw numbers of reads and general run metrics are available to view for libraries [p10-1](https://docs.google.com/spreadsheets/d/1nXT6GJNZ3Gy4PHRYa8s9mExXZpY2Y2ayoZTteQsvvsU/edit#gid=0) and [p10-2](https://docs.google.com/spreadsheets/d/1ZDOYiIGTKL7_cP8gFDDGn78mRlCs8z-NIUwzzwcnTRw/edit#gid=0). A total of 9,219,224 and 10,719,628 reads were sequenced for p10-1 and p10-2 libraries, respectively.  

## file naming

The initial names applied to the **.fastq** files automatically generated by NAU were simplified. The output from NAU for a file followed one of two naming conventions:

- `CONTROL-{SampleName}-xx-EN-USA-2017-076-JF_S{###}_L001_R1_001.fastq.gz` for mock community (positive) and negative control samples, with `SampleName` being specific to each sample, and `###` being either a two or three digit value assigned to the NAU sample sheet (effectively a redundant sample name they apply to each sample; shared with each forward and reverse **.fastq** file pair)
- `NHCS-{SampleName}-xx-VE-USA-2017-076-JF_S{###}_L001_R2_001.fastq.gz` for all true samples, following the same naming scheme as described above

The goal was to produce file names with the following scheme: `{SampleName}_{barcode}_L001_R{#}_001.fastq.gz`.

A series of steps were applied to achieve that, as [described here](https://github.com/devonorourke/guano/blob/master/OahuBird/docs/renaming.md).  

# amptk pipeline

[amptk](https://github.com/nextgenusfs/amptk) is a bioinformatic toolkit which performs all necessary tasks beginning with quality and adapter trimming of raw reads, clustering OTUs, denoising and chimera detection, through to assigning taxonomy to each identified cluster and generating (among other outputs) the list of per-sample taxa represented in the dataset. A full documentation of available parameters used for the program are [detailed here](http://amptk.readthedocs.io/en/latest/index.html).  

Because there are two libraries for this project, initial scripts in this workflow were executed for each library - this is because the index-bleed calculations and potential for contamination are evaluated on a per-library basis. We will ultimately pass individual `amptk illumina`, `amptk cluster`, `amptk filter` and `amptk drop` commands for each library (p10-1 and p10-2); following the final filtering criteria, the two libraries are concatenated and the resulting filtered reads are then re-clustered, filtered, and taxonomy is assigned with the `amptk taxonomy` command.    

## software version history, installation, and virtual environment parameters
A virtual environment was created when completing the installation process. _Recall that `amptk` is written in Python2, not Python3_, thus `conda create -n amptk_env python=3.6` was specified.  

Initial installation proceeded as described in Jon's suggested [installation guide](http://amptk.readthedocs.io/en/latest/#install).  
> A note about versions - in addition to the core Python scripts comprising `amptk`, several dependencies were also installed. Versions used in this analysis include:
- amptk v. 1.1.3-36d7eda
- usearch9 v9.2.64_i86linux32
- usearch10 v10.0.240_i86linux32
- vsearch v2.6.2_linux_x86_64
- remaining python modules and R dependencies were installed via Conda (upgrade/updates with `pip` and/or `conda` performed 16-Mar-2018); install commands were:  

Installation of Python and R dependencies were performed within the virtual environment created:  
```
module purge
module load anaconda/colsa
source activate amptk_env

pip install -U -I biopython natsort pandas numpy matplotlib seaborn edlib biom-format psutil
conda install r-base bioconductor-dada2
conda install r-base bioconductor-dada2
```  

## scripts
While this documentation outlines highlights the core commands used within the `amptk` pipeline, the specific scripts used to execute these commands are saved within [the scripts directory](https://github.com/devonorourke/guano/tree/master/OahuBird/scripts) and named for the `amptk` process in which they were involved. For example, the first script used to apply taxonomy to the data is called `taxonomy.sh`.  

## adapter trimming and PE merging

The first step in the pipeline trims adapters (as a result of the insert length being less than the read length) and then uses USEARCH to merge paired end reads. Orphaned reads are discarded (this typically accounts for less then 2% of the overall number or reads in a sample). A single **.fastq.gz** file is output by concatenating all the individual paired reads with headers modified to specify the sample name. In addition, the **.amptk-demux.log** file documents the proportion of merged reads per sample. These ranged from over a million in the positive controls (ex. ~1.6 million in p10-1 and ~3 million in p10-2) to just 1 read for a single sample (which is ultimately discarded from analysis). There was a significant distribution of numbers of reads for true samples which reflects the stochasticity inherent in amplifying targets from guano extracts, challenges in properly quantifying amplicon vs. primer dimer when pooling, and preference for pure DNA over extract (positive control vs. all else).  

>amptk initially failed because files weren't decompressed properly. This is resolved with the following command:

```
gzip -d *.gz
```

> In addition, an `illumina` directory was created prior to executing the command, and the script was designed to dump the output files into that directory. The entire script was submitted using our SLURM job submission software; pertinent information regarding the amptk-specific arguments were as follows:  

```
amptk illumina \
-i {path/to/input/directory} \
-o trim \
--rescue_forward on \
--require_primer off \
--min_len 160 \
--full_length \
--read_length 300 \
-f GGTCAACAAATCATAAAGATATTGG \
-r GGWACTAATCAATTTCCAAATCC \
--cpus 24 \
--cleanup
```

### dropping samples with less than 50 reads
Before we can cluster our reads within each library, we have to ensure that every sample in our dataset has some minimum number of reads. Jon has set this value to 10 (initially because the DADA2 pipeline (part of the clustering process) requires a read minimum for it's script to function). I've found that I usually apply a `--subtract` value (see the `amptk filter` section below on that filter and how it's applied) of about **20** on most projects; in other words, I find that in most cases I drop out at least 20 reads per sample anyway. We'll start by ensuring that the only samples which are passed into the `amptk cluster` process have _at least 50 reads_, both to fit Jon's requirements, as well as ensure that DADA2 doesn't crash in case the initial small number of reads have very low Phred scores and are filtered out by the initial VSEARCH command that precedes the DADA2 clustering script.  

> Recall that you can determine the number of reads per sample following the `amptk illmina` script (above) by viewing the output of the `.log` file (try `less {filename}.log` to view). We actually don't need to apply the `amptk remove` function to p10-1 because all samples have > 10 reads; however we do need to drop several samples from p10-2, which curiously had many samples with just a few reads.

```
amptk remove \
--input {filename.fq} \
--threshold 50 \
--out {filename}_readfiltd.demux.fq
```

Following the completion of that `amptk remove` script (for p10-2 only) we pass the minimum read filtered samples through to `amptk cluster`. Of note - that dropped **134** samples from p10-2 (that is, 134 samples had less than 50 reads per sample), yet we still retained **10,708,094 reads** out of **10,710,237 total reads**.

## clustering for OTUs

This is a two step process in which the **.fastq** file containing all reads is parsed first using the `DADA2` algorithm creating **iseq** candidate sequences (unique sequences which are not clustered), then these unique sequences are clustered to a 97% similarity using the more traditional `UCLUST` approach. See Jon's documentation describing the differences [here](http://amptk.readthedocs.io/en/latest/clustering.html). In brief, **iseq** values are clustered at a 100% identity, whereas the resulting **OTUs** are clustered at 97% identity, meaning that the **iseq** sequences are more exclusive than the **OTUs**.  

In addition there is a chimera filtering step applied to the data; this requires the installation of the COI database provided by amptk:

```
amptk install -i COI
```

Then execute the clustering with the following code:

> there was a decompresson bug with our system and amptk v1.1.0; I manually decompressed the `.demux.fq.gz` file prior to executing the following code (note `amptk` generates a gzipped file following the completion of `amptk illumina` script above)  

```
amptk dada2 \
--fastq {filePrefix}.demux.fq \
--out {filePrefix} \
--length 180 \
--platform illumina \
--uchime_ref COI
```

The output contains a pair of files which are applied in the next filtering strategy (for index bleed): the `.cluster.otu_table.txt` file which follows a traditional OTU matrix format, as well as the accompanying `.cluster.otus.fa` file which contains the OTU id in the header and the associated sequence. In addiiton, a `.log` file contains information regarding the number of reads and clusters contained within each library. These results are summarized in the following table, grouped by library:

|  | p10-1 | p10-2 |
| --- | --- | --- |
| # OTUs clustered | 3,020 | 1,835 |
| # reads mapped to OTUs | 8,863,391 | 10,299,825 |
| # iSeqs clustered | 12,929 | 3,713 |
| # reads mapped to iSeqs | 8,910,365 | 10,339,310 |

## filtering

Because a mock community was added to each library of this project the proportion of reads that are likely misassigned can be estimated on a per-OTU basis. In brief, we are certain of the OTUs likely to be present in mock community; any additional OTU is the result of index bleed. By calculating the proportion of reads that are present in our mock sample which _shouldn't be there_ we can estimate what fraction of reads (on a per-OTU basis) should be subtracted from all true samples.

This process takes place by applying an initial filtering step that filters reads using the most strict criteria (taking the largest instance of an OTU bleed and applying that percentage to filter across true samples); intermediate files are kept to investigate how the index-bleed is distributed on a per-OTU basis. I have maintained a [separate document](https://github.com/devonorourke/guano/blob/master/Perlut/docs/Perlut_filtering_notes.md) describing the detailed steps used to apply what I feel are the most appropriate filtering strategies for this dataset.   

In brief, this amounted to determining the proportion of index bleed _into the mock community_, the proportion of index bleed _from mock community into true samples_, and identifying any OTUs which were clustered yet not identified in the mock fasta file.  

We initially filter each library separately, then ensure that there are no significant differences arising in terms of suspected contaminants, differences in `index_bleed` values and/or `subtract` values (see the **filtering_notes.md** file for explanations). We observed that there were no large differences among datasets, so the next stage was to combine the reads from both datasets, rename the mock communities as a single item, and then recluster the datasets before performing another round of filtering (filtering on the combined library cluster).

# Starting anew: combining libraries for `p10-1` and `p10-2` to make a master `OahuBird` library
The initial analyses act as a sort of _rough draft_ to approximate the likelihood of contamination in our data as well as get a sense of the distribution of read depths (both on a per-OTU basis and per-sample basis). I'm suspicious of including samples with very low read counts and suspect that some of these OTUs which are creeping in as contaminants are the result of the inclusion of low read-depth samples from the outset. To be more conservative in our analyses we're going to set a minimum read threshold of at least 1000 reads per sample. The earlier analysis allowed the inclusion of any dataset with ~ 50 reads per sample, but one effect is that we can be continuously retaining low-level contaminants in many samples which can be significantly elevating our expected OTU counts.  

We begin the process just as before - but this time we can skip the `amptk illumina` step, and concatenate our two `.demux` files. However because there are two mock community samples among the two lanes of data, we're going to rename those separate mocks as a single mock, then cluster based on the entirety of all mock reads in both datasets (the other option would be to just eliminate one of the two, but this could bias what OTUs we're filtering). We'll then remove any samples with less than 1000 reads, then cluster the resulting OTUs among the samples that survived that filter. We then filter our clustered OTUs as before, but will specifically be interested in identifying which contaminant OTUs remain between those identified individually among the `p10-1` and `p10-2` datasets.  

## combining datasets and renaming mock community headers

Combine our two demultiplexed datasets as follows:
```
cat \
/mnt/lustre/macmaneslab/devon/guano/NAU/OahuBird/illumina/p10-1/trim_p101.demux.fq \
/mnt/lustre/macmaneslab/devon/guano/NAU/OahuBird/illumina/p10-2/trim_p102.demux.fq > \
/mnt/lustre/macmaneslab/devon/guano/NAU/OahuBird/illumina/OahuBird.demux.fq &
```

As a sanity check, you can confirm how many different samples made it into this concatenated file with:
```
grep "^@R_" OahuBird.demux.fq | cut -d '=' -f2 | sort -u | grep -c "^"
```
Which will return a value of **762** - given that we started with **385** and **377** samples in `p10-1` and `p10-2` respectively, we have what we expect in that `OahuBird.demux.fq` file. However, we don't want to include two different mock communities in our filtering - just one - so we'll rename those two mock's as follows:

```
sed -i -e 's/mockIM4p10L1/mockOahuBirdIM4/' -e 's/mockP10L2IM4/mockOahuBirdIM4/' OahuBird.demux.fq &
```

## dropping samples with low read numbers and clustering remaining data

Before cluster that single joint dataset we'll drop out any sample with less than 1000 reads.

```
amptk remove \
--input OahuBird.demux.fq \
--threshold 50 \
--out trimd_OahuBird.demux.fq
```

Notably, the `.log` file points out that we've dropped quite a few samples. We started with **762** samples with at least 1 read, but with the `--threshold 1000` argument set above, we have retained only about half the samples **365** samples are removed, yet we retain **19,807,041** of **19,886,739 total reads** (99.6%). The tradeoff here is the breadth of samples we could investigate with the certainty that the reads we're retaining are likely present due to low-level contamination. Because our first filtering investigations showed that low-level contamination is absolutely in effect, this approach seemed most sensible to me.  

The sequences associated with the remaining samples are then combined into OTUs with `amptk cluster` using the same criteria as with the individual read sets:

```
amptk dada2 \
--fastq trimd_OahuBird.demux.fq \
--out OahuBird \
--length 180 \
--platform illumina \
--uchime_ref COI
```

The output in the `.log` file suggests that we have haven't deviated from the expected number of OTUs relative to initial findings - they're about the sum of the two libraries - but this appears to me at least a bit unexpected, as it suggests these two runs have a lot of independent sequences (whereas given they were all from the same project, I'd have suspected there would be a reduction in the overall number of OTUs once we clustered the two datasets). The table below compares the initial findings of the separately clustered libraries with our joint library:  

|  | p10-1 | p10-2 | OahuBird (all)
| --- | --- | --- | --- |
| # OTUs clustered | 3,020 | 1,835 | 4,177 |
| # reads mapped to OTUs | 8,863,391 | 10,299,825 | 19,109,186 |
| # iSeqs clustered | 12,929 | 3,713 | 15,648 |
| # reads mapped to iSeqs | 8,910,365 | 10,339,310 | 19,180,369 |

## Filtering the joint library  
We employ the same filtering strategies as discussed in detail in the supplementary [filtering document](https://github.com/devonorourke/guano/blob/master/OahuBird/docs/filtering.md). In summary, after applying a series of filters, we retain **1,529 OTUs** and **13,950,987 reads**. This value is considerably less than the initial library (both in terms of OTUs and reads); the read drop is expected because of the fact that so many reads were part of the mock community - these were included in our estimates of OTUs and reads in the table above, yet are not included in our final dataset after filtering. The number of OTUs dropping isn't surprising - in most datasets these OTU values are initially quite high and are due to many low-abundant reads due to index-bleed and contamination; once we've accounted for those factors many OTUs are discarded.

### Optional filtering - work in progress
Jon has incorporated a third party software program that further refines our OTU estimates. While our initial filtering removes lots of OTUs we believe to be in low abundance due to index-bleed and/or contamination, it's also possible that these OTUs are in low abundance due to PCR error that creates unique OTUs when they should really be clustered together. The [LULU package](https://github.com/tobiasgf/lulu), is an algorithm for further clustering - see [their paper](https://www.nature.com/articles/s41467-017-01312-x) for complete details. The usage of this software is appropriate within our amptk workflow following our filtering, but should be completed before taxonomy assignment; usage is outlined within the [amptk documentation](http://amptk.readthedocs.io/en/latest/commands.html#amptk-lulu). I've tried to simplify what is happening in the following bullets, but see [LULU's github page](https://github.com/tobiasgf/lulu#the-algorithm) for even more information about the algorithm's process:  

- Though there are several parameters to potentially test, just one struck me as particularly relevant to toying with: the `--min_match` setting. LULU works in a series of steps; following sorting the OTU table by abundance (prioritized in terms numbers of detections for an OTU among samples first, then in terms of reads per OTU), the dataset is put through a pairwise percent dissimilarity test (BLASTn approach used by default) - these percent (dis)similarities are then used in the R LULU script which will only test for parent-daughter relationships _based on the `--min_match` value you specify_. Thus the default **84** percent dissimilarity seemed really low to me; a more conservative estimate of 95% would essentially specify that up to 5% of the variance between a pair of sequences is indicative of either PCR errors and/or intragenic variance among true OTUs within a species. While we've already clustered at 97% with VSEARCH (during the `amptk clust` process), this additional percent identity allows us to then label these _potentially related_ clusters.  
- We then move along to the next major step where potentially related clusters are queried for their **cooccurrence** - this value is set by the `--minimum_relative_cooccurence` argument. This is a percent which reflects how likely you think these values _potentially related clusters are to cooccur_. We would think that if they are truly PCR errors or intragenic variation, then they should **often** cooccur. We set this to a fairly high value of 95%, meaning that we'd only expect them to be separated 5% of the time. Notably, this measure only is stipulated upon the daughter strand, so you can have the dominant parent OTU in lots of samples without the daughter, but you can't have lots of daugther samples without the parent. I haven't changed this setting in our testing because I have not empirically tested this with a positive control. A value of **95** percent is used by default.  
- By default, the parent OTU has to be more abundant than the daughter strand in all cases. However you can specify that an error _needs to have lower abundance on average_. In such a situation, all flagged daughter strands may not necessarily have the lowest read abundance, but their average might. This would be a more liberal option which I'm avoiding changing from their default.  
- An updated OTU table is provided as are several other output files. I'm interested in further investigating whether our most abundant OTUs going into the filtering have the most daughter OTUs merged. I haven't done that yet with this data, but I did apply a 2x2 filtering criteria:  
  - the input OTU table and fasta file were from the unfiltered `amptk clust` output (appended **Unfilt** in the output) or the OTU table and fasta file from the final `amptk filter` script which included several dropped OTUs, an `index-bleed` filter, and a `subtract` filter.  
  - the `--min_match` setting was used at the default **84** value or a more conservative **95** percent similarity  

The following script is an example of one of the four script used to make the output summary provided in the table below:  
> note prior to executing this script, a `lulu` directory was created within the parent directory for this project, and separate subdirectories within that parent directory were created for each filtering paradigm  
> just one example script is shown, using the unfiltered OTU table/fasta files, and a modified `--min_match` setting  

```
amptk lulu \
-i /mnt/lustre/macmaneslab/devon/guano/NAU/OahuBird/clust/all/OahuBird.final.csv \
-f /mnt/lustre/macmaneslab/devon/guano/NAU/OahuBird/clust/all/OahuBird.filtered.otus.fa \
-o defaultUnfilt \
--min_match 95
```

The simplest comparison to make with using LULU is how many OTUs remain following this additional clustering process; we started with **1,529** OTUs in our filtered dataset, yet following the `lulu` script we find that LULU has merged a different number of OTUs depending on which script parameters we applied (both in terms of the input OTU tables as well as the percent identity parameter passed within the LULU program itself). The following data table summarizes the results:  
> Note that the *Unfiltered* OTU values are slightly inflated by the 24 mock community-associated OTUs  

| OTU dataset | min_match value | # preLULU OTUs | # postLULU OTUs | # OTUs LULU clustered |
| --- | --- | --- | --- | --- |
| Filtered | 84 | 1,529 | 1,123 | 406 |
| Filtered | 95 | 1,529 | 1,416 | 113 |
| UNfiltered | 84 | 4,178 | 4,177 | 1 |
| UNfiltered | 95 | 4,178 | 4,177 | 1 |

What was expected was a reduction in number of OTUs curated by LULU when we specify a higher `--min_match` value, and indeed that's what we see in our **filtered** dataset. Specifying a 95% identity for consideration of cluster inclusion only **reduced our number of OTUs by about 8%**, whereas the default parameters **reduced our OTUs by almost 27%**. That's a big difference, but I don't see how we can argue using one parameter value over another without any empirical evidence suggesting that this kind of inclusion is warranted. What wasn't expected was the lack of clustering int he **unifiltered** dataset; in the LULU paper they describe how their curation had the greatest impact on the data with most OTUs and we're seeing the opposite.  

Clearly, further work is necessary to better understand how to best use this program; for the time being we'll keep the curated OTU table from the **filtered** dataset as well as the **filtered** OTU table which wasn't curated through LULU. The **unfiltered** data will not be used in our forthcoming taxonomic analyses. Notably, the reduction in OTUs can then further impact our diversity measures (should we choose to evaluate those), but those aren't the next step in our workflow - that's assigning taxonomy to the OTU table and fasta files. We'll update those, and then proceed with ecology measures in detail later in the workflow.   

## taxonomy assignment
As described in the [amptk taxonomy](http://amptk.readthedocs.io/en/latest/taxonomy.html) section, the database used to assign taxonomy is derived from the Barcode of Life Database ([BOLD](http://v4.boldsystems.org/)). The sequences present in the database we're using are the result of two sequential clustering processes.
- BOLD's BIN data serve as the initial sequence material. These sequences themselves are initially derived from [a clustering process](http://v4.boldsystems.org/index.php/Public_BarcodeIndexNumber_Home).
- The BIN sequences are then clustered locally by amptk to 99% identity. These data are further processed to train the UTAX program which can be used in taxonomic assignment/prediction.  

> This database was updated as of 14-sept-2017, following the [release](https://github.com/nextgenusfs/amptk/releases/tag/1.0.0) of amptk v-1.0.0.  
> Complete database download is [available here](https://osf.io/4xd9r/files/)

Taxonomy was explored using the _hybrid_ approach (default in amptk). See Jon's description of the steps used in his documentation at the link above for further details; notably, other taxonomic assignment options are available within `amptk`. I had to create a new mapping file because we combined reads from two datasets by concatenating the `amptk illumina` output (rather than rerunning it again from all the reads at once). This mapping file was then passed to the `amptk taxonomy` command below. To create the mapping file we want:
> note a **taxonomy** directory was created within the parent directory of this project    

```
## generate the sample list we're sorting through:
cd /mnt/lustre/macmaneslab/devon/guano/NAU/OahuBird/filtd/all/
head -1 OahuBird.final.txt | tr '\t' '\n' | sed 's/#OTU ID/#SampleID/' > wanted_list.txt

## run R script to:
## (1) combine initial mapping files from both libraries
## (2) subset from this master mapping file only samples present in our filtered OTU tables (rows matching the `wanted_list.txt`)  

R
p101map <- read.csv("/mnt/lustre/macmaneslab/devon/guano/NAU/OahuBird/illumina/p10-1/trim_p101.mapping_file.txt",sep='\t', header=TRUE)
colnames(p101map)[1] <- "#SampleID"
p102map <- read.csv("/mnt/lustre/macmaneslab/devon/guano/NAU/OahuBird/illumina/p10-2/trim_p102.mapping_file.txt",sep='\t', header=TRUE)
colnames(p102map)[1] <- "#SampleID"
allmap <- rbind(p101map,p102map)
wanted <- read.csv("/mnt/lustre/macmaneslab/devon/guano/NAU/OahuBird/filtd/all/wanted_list.txt",sep='\t',header=TRUE)
colnames(wanted) <- "#SampleID"
library(dplyr)
filtmap <- right_join(allmap, wanted)
write.table(filtmap, file = "/mnt/lustre/macmaneslab/devon/guano/NAU/OahuBird/taxonomy/FilteredMappingFile.txt",sep='\t', quote = F, row.names=F)
```

Then use this mapping file (**FilteredMappingFile.txt**) for each of the datasets you're assigning taxonomy to (both the _LULU-curated_ as well as _filtered_ dataset without LULU curation). I'm terming each of these datasets as either LULU filtered or not, so they are discussed as **lulu** or **NOlulu** herein:  
> Shown below reflects one of the two similar set of `amptk taxonomy` commands used (in this case, for the LULU curated and filtered dataset); for the _filtered but not curated_ dataset (**NOlulu**), replace the input otu table and fasta files  
> note that subdirectories within the **taxonomy** parent directory were created for each of the unique otu tables used in this analysis  

```
amptk taxonomy \
-i /mnt/lustre/macmaneslab/devon/guano/NAU/OahuBird/lulu/pid95/pid95.lulu.otu_table.txt \
--fasta /mnt/lustre/macmaneslab/devon/guano/NAU/OahuBird/lulu/pid95/pid95.lulu.otus.fa \
--out OahuBird_lulu_h \
--db COI \
--method hybrid \
--mapping_file FilteredMappingFile.txt
```

I termed the two output datasets `OahuBird_lulu_h` for LULU-curated OTU table, while I appended a `Oahubird_h` prefix for the non-LULU curated OTU table (the reason being that LULU was an additional filter in addition to the filtering methods already applied to the **NOlulu** dataset). Each of the datasets contained multiple output files; I've uploaded the `.fasta` sequence and OTU `.txt` files with taxonomic information to [the `/data/amptk` directory within this Github repo](https://github.com/devonorourke/guano/tree/master/OahuBird/data/amtpk) as well as a `.biom` file which should be useful for additional analyses in other software packages if warranted.    

> Note that a value of 0 ("absence") could mean a variety of different things:  
> - it could be that the OTU is not truly in the sample of guano
> - it could be because an OTU was present but not amplified and sequenced  
> - it could be the OTU was sequenced but there wasn't enough reads to pass our filters (with `--index_bleed` and `--subtract` arguments in `amptk filter`))

 # Further analyses

 An R script was then used to manipulate the output `Perlut.otu_table.taxonomy.txt` file which includes both further data filtering, as well as the calculations for frequency tables and visualizations - [see here](https://github.com/devonorourke/guano/blob/master/Perlut/docs/OTUanalysis.R).  

 > One such data filtering taking place here is the removal of reads associated with the mock community.

Output from this R script are placed within [the `/data/Routput` directory within this Github repo](https://github.com/devonorourke/guano/tree/master/Perlut/data/Routput).  
